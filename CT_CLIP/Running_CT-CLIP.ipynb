{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e783e94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'monai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydicom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Compose, LoadImage, EnsureChannelFirst, Spacing, ScaleIntensity\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_dicom_series\u001b[39m(dicom_dir):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Get list of DICOM files\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     dicom_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dicom_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dicom_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.dcm\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'monai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from monai.transforms import Compose, LoadImage, EnsureChannelFirst, Spacing, ScaleIntensity\n",
    "\n",
    "def load_dicom_series(dicom_dir):\n",
    "    # Get list of DICOM files\n",
    "    dicom_files = [os.path.join(dicom_dir, f) for f in os.listdir(dicom_dir) if f.endswith('.dcm')]\n",
    "    dicom_files.sort()  # Ensure correct slice order (may need to sort by InstanceNumber)\n",
    "\n",
    "    # Read DICOM files\n",
    "    slices = [pydicom.dcmread(f) for f in dicom_files]\n",
    "    slices.sort(key=lambda x: x.InstanceNumber)  # Sort by instance number\n",
    "\n",
    "    # Stack slices into 3D volume\n",
    "    volume = np.stack([s.pixel_array for s in slices], axis=-1)  # Shape: (H, W, Z)\n",
    "\n",
    "    return volume\n",
    "\n",
    "# Paths to your DICOM folders\n",
    "dicom_dirs = [\n",
    "    r\"C:\\Users\\jaspe\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\TEST CT SCANS\\thx ax mip 10 8\",\n",
    "    r\"C:\\Users\\jaspe\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\TEST CT SCANS\\thx bb cor 3 3\",\n",
    "    r\"C:\\Users\\jaspe\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\TEST CT SCANS\\thx bb sag 3 3\"\n",
    "]\n",
    "\n",
    "# Load each series\n",
    "volumes = [load_dicom_series(d) for d in dicom_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f20bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type cxr-bert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at microsoft/BiomedVLP-CXR-BERT-specialized and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3623878656 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     17\u001b[0m text_transformer \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/BiomedVLP-CXR-BERT-specialized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize vision transformer\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# The shape mismatch for to_visual_latent.weight ([512, 294912]) suggests a larger output dimension\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate expected dim_image: 294912 / (256 // 32) ** 2 = 294912 / 64 = 4608 tokens\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# This implies a different patch size or additional features\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m vision_transformer \u001b[38;5;241m=\u001b[39m VisionTransformer(\n\u001b[0;32m     24\u001b[0m     dim\u001b[38;5;241m=\u001b[39mdim_image,\n\u001b[0;32m     25\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mvisual_image_size,\n\u001b[0;32m     26\u001b[0m     patch_size\u001b[38;5;241m=\u001b[39mvisual_patch_size,  \u001b[38;5;66;03m# May need adjustment (e.g., smaller patch size)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     channels\u001b[38;5;241m=\u001b[39mchannels,\n\u001b[0;32m     28\u001b[0m     depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m     29\u001b[0m     heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     30\u001b[0m     dim_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     31\u001b[0m     patch_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize CT-CLIP\u001b[39;00m\n\u001b[0;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m CTCLIP(\n\u001b[0;32m     36\u001b[0m     image_encoder\u001b[38;5;241m=\u001b[39mvision_transformer,\n\u001b[0;32m     37\u001b[0m     text_encoder\u001b[38;5;241m=\u001b[39mtext_transformer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     multiview_loss_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     52\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20203686\\Documents\\GitHub\\CT-CLIP\\CT_CLIP\\ct_clip\\ct_clip.py:353\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[1;34m(self, dim, image_size, patch_size, channels, patch_dropout, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m (image_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m patch_size) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    349\u001b[0m patch_dim \u001b[38;5;241m=\u001b[39m channels \u001b[38;5;241m*\u001b[39m patch_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m    352\u001b[0m     Rearrange(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c (h p1) (w p2) -> b (h w) (p1 p2 c)\u001b[39m\u001b[38;5;124m'\u001b[39m, p1 \u001b[38;5;241m=\u001b[39m patch_size, p2 \u001b[38;5;241m=\u001b[39m patch_size),\n\u001b[1;32m--> 353\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(patch_dim, dim)\n\u001b[0;32m    354\u001b[0m )\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(num_patches, dim)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_dropout \u001b[38;5;241m=\u001b[39m PatchDropout(patch_dropout)\n",
      "File \u001b[1;32mc:\\Users\\20203686\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3623878656 bytes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ct_clip.ct_clip import CTCLIP, VisionTransformer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Define model parameters\n",
    "dim_text = 768  # Matches BiomedVLP-CXR-BERT-specialized output dimension\n",
    "dim_image = 294912  # Adjust based on VisionTransformer output\n",
    "dim_latent = 512\n",
    "num_text_tokens = 28897\n",
    "text_seq_len = 512\n",
    "visual_image_size = 256\n",
    "visual_patch_size = 32  # Adjust if needed (see below)\n",
    "channels = 3\n",
    "\n",
    "# Initialize text transformer using BertModel\n",
    "text_transformer = BertModel.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized')\n",
    "\n",
    "# Initialize vision transformer\n",
    "# The shape mismatch for to_visual_latent.weight ([512, 294912]) suggests a larger output dimension\n",
    "# Calculate expected dim_image: 294912 / (256 // 32) ** 2 = 294912 / 64 = 4608 tokens\n",
    "# This implies a different patch size or additional features\n",
    "vision_transformer = VisionTransformer(\n",
    "    dim=dim_image,\n",
    "    image_size=visual_image_size,\n",
    "    patch_size=visual_patch_size,  # May need adjustment (e.g., smaller patch size)\n",
    "    channels=channels,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    dim_head=64,\n",
    "    patch_dropout=0.5\n",
    ")\n",
    "\n",
    "# Initialize CT-CLIP\n",
    "model = CTCLIP(\n",
    "    image_encoder=vision_transformer,\n",
    "    text_encoder=text_transformer,\n",
    "    dim_text=dim_text,\n",
    "    dim_image=dim_image,\n",
    "    dim_latent=dim_latent,\n",
    "    num_text_tokens=num_text_tokens,\n",
    "    text_seq_len=text_seq_len,\n",
    "    visual_image_size=visual_image_size,\n",
    "    visual_patch_size=visual_patch_size,\n",
    "    channels=channels,\n",
    "    use_mlm=False,\n",
    "    use_visual_ssl=False,\n",
    "    visual_ssl_type='simsiam',\n",
    "    text_ssl_loss_weight=0.05,\n",
    "    image_ssl_loss_weight=0.05,\n",
    "    multiview_loss_weight=0.1\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load pretrained weights\n",
    "state_dict = torch.load(\n",
    "    r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\CT-CLIP\\CT_VocabFine_v2.pt\",\n",
    "    map_location=device\n",
    ")\n",
    "model.load_state_dict(state_dict, strict=False)  # Use strict=False to ignore missing/unexpected keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b50850",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b14fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
