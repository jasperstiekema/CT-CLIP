{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64a12fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch shape: torch.Size([3, 3, 256, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "\n",
    "def load_dicom_volume(dicom_folder, target_size=(256, 256, 256), channels=3):\n",
    "    # Get all .dcm files in the folder\n",
    "    dicom_files = sorted(glob.glob(os.path.join(dicom_folder, \"*.dcm\")))\n",
    "    if not dicom_files:\n",
    "        raise ValueError(f\"No DICOM files found in {dicom_folder}\")\n",
    "\n",
    "    # Read DICOM files and stack slices\n",
    "    slices = [pydicom.dcmread(f).pixel_array for f in dicom_files]\n",
    "    volume = np.stack(slices, axis=-1)  # Shape: (height, width, depth)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    volume = (volume - volume.min()) / (volume.max() - volume.min() + 1e-6)\n",
    "\n",
    "    # Convert single-channel to 3 channels if needed\n",
    "    if channels == 3:\n",
    "        volume = np.repeat(volume[..., np.newaxis], 3, axis=-1)  # Shape: (height, width, depth, channels)\n",
    "    else:\n",
    "        volume = volume[..., np.newaxis]  # Shape: (height, width, depth, 1)\n",
    "\n",
    "    # Resize to target size (requires interpolation, e.g., using scipy or torchvision)\n",
    "    from scipy.ndimage import zoom\n",
    "    factors = [t / s for t, s in zip(target_size, volume.shape[:-1])]\n",
    "    volume = zoom(volume, factors + [1], order=1)  # Linear interpolation, preserve channel dimension\n",
    "\n",
    "    # Convert to PyTorch tensor and adjust dimensions\n",
    "    volume = torch.tensor(volume, dtype=torch.float32).permute(3, 0, 1, 2)  # Shape: (channels, height, width, depth)\n",
    "    return volume\n",
    "\n",
    "# Example: Load all volumes from subfolders\n",
    "data_dir = r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\TEST CT SCANS\"\n",
    "subfolders = [f for f in Path(data_dir).iterdir() if f.is_dir()]\n",
    "image_tensors = []\n",
    "\n",
    "for folder in subfolders:\n",
    "    volume = load_dicom_volume(str(folder))\n",
    "    image_tensors.append(volume)\n",
    "\n",
    "# Stack into a batch\n",
    "image_batch = torch.stack(image_tensors)  # Shape: (batch, channels, height, width, depth)\n",
    "print(f\"Loaded batch shape: {image_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe21f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\CT-CLIP\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CXRBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ct_clip import CTCLIP\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize CT-CLIP model\n",
    "model = CTCLIP(\n",
    "    dim_text=512,              # Text embedding dimension\n",
    "    dim_image=512,             # Image embedding dimension\n",
    "    dim_latent=512,            # Latent space dimension\n",
    "    num_text_tokens=28897,     # Vocabulary size for text\n",
    "    text_seq_len=512,          # Max text sequence length\n",
    "    visual_image_size=256,     # Image size (matches your 256x256x256 input)\n",
    "    visual_patch_size=32,      # Patch size for VisionTransformer\n",
    "    channels=3,                # Number of channels (matches your 3-channel input)\n",
    "    use_mlm=False,             # Disable MLM since no text reports\n",
    "    use_visual_ssl=False,       # Disable visual SSL for inference\n",
    "    text_has_cls_token=False,  # Adjust based on model checkpoint\n",
    "    visual_has_cls_token=False # Adjust based on model checkpoint\n",
    ").to(device)\n",
    "\n",
    "# Load pretrained weights\n",
    "checkpoint_path = r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\CT-CLIP\\CT_VocabFine_v2.pt\"\n",
    "model.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540f647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CXRBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', do_lower_case=True)\n",
    "\n",
    "# Create placeholder text for the batch (3 images)\n",
    "texts = [\"CT scan\" for _ in range(3)]  # Match batch size of images\n",
    "text_tokens = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d45e68",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextTransformer.forward() got an unexpected keyword argument 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     text_latents, image_latents, encoded_images = model(\n\u001b[32m     10\u001b[39m         text=text_tokens,           \u001b[38;5;66;03m# Placeholder text\u001b[39;00m\n\u001b[32m     11\u001b[39m         image=image_batch,         \u001b[38;5;66;03m# Your CT scan batch\u001b[39;00m\n\u001b[32m     12\u001b[39m         device=device,\n\u001b[32m     13\u001b[39m         return_loss=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     14\u001b[39m         return_encodings=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     15\u001b[39m         return_latents=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Print shapes\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage latents shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_latents.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\CT-CLIP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20203686\\Documents\\GitHub\\CT-CLIP\\CT_CLIP\\ct_clip\\ct_clip.py:685\u001b[39m, in \u001b[36mCTCLIP.forward\u001b[39m\u001b[34m(self, text, image, device, return_loss, return_encodings, return_latents, freeze_image_encoder, freeze_text_encoder, text_to_image, aug_text, aug_image)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_encode_without_mask:\n\u001b[32m    682\u001b[39m     text_args = (*text_args, text_mask)\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m text_embeddings = \u001b[38;5;28mself\u001b[39m.text_transformer(text.input_ids, attention_mask = text.attention_mask )\n\u001b[32m    686\u001b[39m enc_text = text_embeddings[\u001b[32m0\u001b[39m]\n\u001b[32m    688\u001b[39m \u001b[38;5;66;03m# depending on whether text is using causal mask, post process, moving eos token to the first position\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\CT-CLIP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[31mTypeError\u001b[39m: TextTransformer.forward() got an unexpected keyword argument 'attention_mask'"
     ]
    }
   ],
   "source": [
    "# Assume image_batch is already prepared with shape [3, 3, 256, 256, 256]\n",
    "image_batch = image_batch.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    text_latents, image_latents, encoded_images = model(\n",
    "        text=text_tokens,           # Placeholder text\n",
    "        image=image_batch,         # Your CT scan batch\n",
    "        device=device,\n",
    "        return_loss=False,\n",
    "        return_encodings=False,\n",
    "        return_latents=True\n",
    "    )\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Image latents shape: {image_latents.shape}\")\n",
    "print(f\"Encoded images shape: {encoded_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad990da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT-CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
