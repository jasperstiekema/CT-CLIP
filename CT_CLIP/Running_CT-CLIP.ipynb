{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59737f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pydicom\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import zoom\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f462e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CT-CLIP imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import CT-CLIP - adjust path as needed\n",
    "import sys\n",
    "sys.path.append(r'CT_CLIP')  # Adjust this path to your CT-CLIP directory\n",
    "\n",
    "try:\n",
    "    from ct_clip.ct_clip import CTCLIP\n",
    "    print(\"✓ CT-CLIP imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Error importing CT-CLIP: {e}\")\n",
    "    print(\"Make sure the CT_CLIP path is correct and the module is installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4c8a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DICOM loading function defined\n"
     ]
    }
   ],
   "source": [
    "def load_dicom_series(folder_path):\n",
    "    \"\"\"\n",
    "    Load a series of DICOM files from a folder and create a 3D volume.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing DICOM files\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 3D CT volume (H, W, D)\n",
    "    \"\"\"\n",
    "    dicom_files = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Get all .dcm files\n",
    "    dcm_files = list(folder.glob(\"*.dcm\"))\n",
    "    if not dcm_files:\n",
    "        raise ValueError(f\"No .dcm files found in {folder_path}\")\n",
    "    \n",
    "    print(f\"Found {len(dcm_files)} DICOM files in {folder.name}\")\n",
    "    \n",
    "    # Load DICOM files\n",
    "    for dcm_file in dcm_files:\n",
    "        try:\n",
    "            ds = pydicom.dcmread(dcm_file)\n",
    "            dicom_files.append(ds)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {dcm_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not dicom_files:\n",
    "        raise ValueError(f\"No valid DICOM files could be loaded from {folder_path}\")\n",
    "    \n",
    "    # Sort by slice location or instance number\n",
    "    try:\n",
    "        dicom_files.sort(key=lambda x: float(x.SliceLocation))\n",
    "        print(\"Sorted by SliceLocation\")\n",
    "    except (AttributeError, ValueError):\n",
    "        try:\n",
    "            dicom_files.sort(key=lambda x: int(x.InstanceNumber))\n",
    "            print(\"Sorted by InstanceNumber\")\n",
    "        except (AttributeError, ValueError):\n",
    "            print(\"Warning: Could not sort DICOM files by slice location or instance number\")\n",
    "    \n",
    "    # Extract pixel arrays and create 3D volume\n",
    "    slices = []\n",
    "    for ds in dicom_files:\n",
    "        # Apply DICOM rescaling\n",
    "        pixel_array = ds.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Apply rescale slope and intercept if available (convert to Hounsfield Units)\n",
    "        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
    "            pixel_array = pixel_array * ds.RescaleSlope + ds.RescaleIntercept\n",
    "        \n",
    "        slices.append(pixel_array)\n",
    "    \n",
    "    # Stack slices to create 3D volume\n",
    "    volume = np.stack(slices, axis=-1)  # Shape: (H, W, D)\n",
    "    \n",
    "    print(f\"Loaded CT volume with shape: {volume.shape}\")\n",
    "    return volume\n",
    "\n",
    "print(\"✓ DICOM loading function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6bf5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CT scan folders:\n",
      "  - THX AX MIP 10 8\n",
      "  - THX BB COR 3 3\n",
      "  - THX BB SAG 3 3\n",
      "\n",
      "Testing DICOM loading with: THX AX MIP 10 8\n",
      "Found 45 DICOM files in THX AX MIP 10 8\n",
      "Sorted by SliceLocation\n",
      "Loaded CT volume with shape: (512, 512, 45)\n",
      "✓ Successfully loaded CT volume with shape: (512, 512, 45)\n",
      "✓ HU range: [-1024.0, 3071.0]\n"
     ]
    }
   ],
   "source": [
    "# Configuration - adjust these paths\n",
    "base_folder = r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\TEST CT SCANS\"\n",
    "\n",
    "# Get all subdirectories (each containing one CT scan)\n",
    "ct_scan_folders = [d for d in Path(base_folder).iterdir() if d.is_dir()]\n",
    "\n",
    "if not ct_scan_folders:\n",
    "    print(f\"No subdirectories found in {base_folder}\")\n",
    "    print(\"Make sure your CT scans are organized in separate folders\")\n",
    "else:\n",
    "    print(f\"Found {len(ct_scan_folders)} CT scan folders:\")\n",
    "    for folder in ct_scan_folders:\n",
    "        print(f\"  - {folder.name}\")\n",
    "    \n",
    "    # Test loading the first CT scan\n",
    "    test_folder = ct_scan_folders[0]\n",
    "    print(f\"\\nTesting DICOM loading with: {test_folder.name}\")\n",
    "    \n",
    "    try:\n",
    "        ct_volume = load_dicom_series(test_folder)\n",
    "        print(f\"✓ Successfully loaded CT volume with shape: {ct_volume.shape}\")\n",
    "        print(f\"✓ HU range: [{ct_volume.min():.1f}, {ct_volume.max():.1f}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading DICOM: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a9d789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing function defined\n"
     ]
    }
   ],
   "source": [
    "def preprocess_ct_volume(volume, target_size=(224, 224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess CT volume for CT-CLIP model.\n",
    "    Based on typical medical image preprocessing practices.\n",
    "    \n",
    "    Args:\n",
    "        volume (numpy.ndarray): Input CT volume (H, W, D)\n",
    "        target_size (tuple): Target size (H, W, D)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed volume ready for model input\n",
    "    \"\"\"\n",
    "    print(f\"Original volume shape: {volume.shape}\")\n",
    "    print(f\"Volume HU range: [{volume.min():.1f}, {volume.max():.1f}]\")\n",
    "    \n",
    "    # Apply windowing (typical for CT scans)\n",
    "    # Using a general soft tissue window: center=40, width=350\n",
    "    window_center, window_width = 40, 350\n",
    "    window_min = window_center - window_width // 2\n",
    "    window_max = window_center + window_width // 2\n",
    "    \n",
    "    # Apply windowing\n",
    "    volume_windowed = np.clip(volume, window_min, window_max)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    volume_norm = (volume_windowed - window_min) / (window_max - window_min)\n",
    "    \n",
    "    print(f\"After windowing and normalization: [{volume_norm.min():.3f}, {volume_norm.max():.3f}]\")\n",
    "    \n",
    "    # Resize volume to target size\n",
    "    current_shape = volume_norm.shape\n",
    "    zoom_factors = [target_size[i] / current_shape[i] for i in range(3)]\n",
    "    print(f\"Zoom factors: {zoom_factors}\")\n",
    "    \n",
    "    volume_resized = zoom(volume_norm, zoom_factors, order=1)\n",
    "    print(f\"Resized volume shape: {volume_resized.shape}\")\n",
    "    \n",
    "    # Convert to tensor and add batch and channel dimensions\n",
    "    # CT-CLIP expects shape: (batch, channels, depth, height, width)\n",
    "    volume_tensor = torch.from_numpy(volume_resized).float()\n",
    "    volume_tensor = volume_tensor.permute(2, 0, 1)  # (D, H, W)\n",
    "    volume_tensor = volume_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, D, H, W)\n",
    "    \n",
    "    print(f\"Final tensor shape: {volume_tensor.shape}\")\n",
    "    return volume_tensor\n",
    "\n",
    "print(\"✓ Preprocessing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43566c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original volume shape: (512, 512, 45)\n",
      "Volume HU range: [-1024.0, 3071.0]\n",
      "After windowing and normalization: [0.000, 1.000]\n",
      "Zoom factors: [0.4375, 0.4375, 4.977777777777778]\n",
      "Resized volume shape: (224, 224, 224)\n",
      "Final tensor shape: torch.Size([1, 1, 224, 224, 224])\n",
      "✓ Successfully preprocessed CT volume\n",
      "✓ Ready for model input with shape: torch.Size([1, 1, 224, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Test preprocessing on the loaded CT volume\n",
    "if 'ct_volume' in locals():\n",
    "    try:\n",
    "        preprocessed_volume = preprocess_ct_volume(ct_volume)\n",
    "        print(f\"✓ Successfully preprocessed CT volume\")\n",
    "        print(f\"✓ Ready for model input with shape: {preprocessed_volume.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error preprocessing CT volume: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"✗ No CT volume loaded. Run the DICOM loading cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409db83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CT-CLIP model...\n",
      "Loading BiomedVLP-CXR-BERT-specialized tokenizer and model...\n",
      "✓ BiomedVLP tokenizer and model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CXRBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\CT-CLIP\\CT_VocabFine_v2.pt\n",
      "✓ Pretrained weights loaded successfully\n",
      "✓ Model and tokenizer loaded successfully!\n",
      "✓ CT-CLIP model is on device: cuda:0\n",
      "✓ BiomedVLP model is on device: cuda:0\n",
      "\n",
      "Testing text embeddings...\n",
      "✓ Text embeddings shape: torch.Size([3, 128])\n",
      "✓ Similarity matrix shape: torch.Size([3, 3])\n",
      "✓ Similarity matrix:\n",
      "tensor([[ 1.0000,  0.7456, -0.1916],\n",
      "        [ 0.7456,  1.0000, -0.4709],\n",
      "        [-0.1916, -0.4709,  1.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from ct_clip.ct_clip import CTCLIP, TextTransformer, VisionTransformer\n",
    "\n",
    "def load_ctclip_model(\n",
    "    model_path=r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\CT-CLIP\\CT_VocabFine_v2.pt\",\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Load CT-CLIP model with proper BiomedVLP tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to pretrained model weights (optional)\n",
    "        device (str): Device to use\n",
    "        \n",
    "    Returns:\n",
    "        CTCLIP model, tokenizer, biomed_model (for text embeddings)\n",
    "    \"\"\"\n",
    "    print(\"Initializing CT-CLIP model...\")\n",
    "    \n",
    "    # Load the specialized biomedical tokenizer and model\n",
    "    print(\"Loading BiomedVLP-CXR-BERT-specialized tokenizer and model...\")\n",
    "    url = \"microsoft/BiomedVLP-CXR-BERT-specialized\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(url, trust_remote_code=True)\n",
    "        biomed_model = AutoModel.from_pretrained(url, trust_remote_code=True)\n",
    "        biomed_model.to(device)\n",
    "        biomed_model.eval()\n",
    "        print(\"✓ BiomedVLP tokenizer and model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading BiomedVLP model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Define text encoder (TextTransformer from ct_clip.py)\n",
    "    text_encoder = TextTransformer(\n",
    "        dim=768,  # Matches dim_text\n",
    "        num_tokens=28897,  # Matches default in CTCLIP\n",
    "        max_seq_len=256,  # Matches text_seq_len\n",
    "        depth=12,  # Matches checkpoint's layer count (inferred from text_transformer.encoder.layer.11)\n",
    "        heads=12,  # Matches BERT-style attention\n",
    "        dim_head=64,\n",
    "        rotary_pos_emb=False,  # Default in CTCLIP\n",
    "        causal=False  # Default in CTCLIP\n",
    "    )\n",
    "    \n",
    "    # Define image encoder (VisionTransformer from ct_clip.py)\n",
    "    image_encoder = VisionTransformer(\n",
    "        dim=768,  # Matches expected output dimension\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        channels=1,  # Grayscale CT scans\n",
    "        depth=12,  # Matches visual_enc_depth\n",
    "        heads=12,  # Matches visual_heads\n",
    "        dim_head=64,  # Matches visual_dim_head\n",
    "        patch_dropout=0.5  # Default in CTCLIP\n",
    "    )\n",
    "    \n",
    "    # Initialize CTCLIP model\n",
    "    model = CTCLIP(\n",
    "        image_encoder=image_encoder,\n",
    "        text_encoder=text_encoder,\n",
    "        dim_image=294912,  # Matches checkpoint's to_visual_latent.weight\n",
    "        dim_text=768,\n",
    "        dim_latent=512,\n",
    "        num_text_tokens=28897,\n",
    "        text_enc_depth=12,\n",
    "        text_seq_len=256,\n",
    "        text_heads=12,\n",
    "        text_dim_head=64,\n",
    "        text_has_cls_token=False,\n",
    "        text_pad_id=0,\n",
    "        text_rotary_pos_emb=False,\n",
    "        text_causal_mask=False,\n",
    "        visual_enc_depth=12,\n",
    "        visual_heads=12,\n",
    "        visual_dim_head=64,\n",
    "        visual_image_size=224,\n",
    "        visual_patch_size=16,\n",
    "        visual_patch_dropout=0.5,\n",
    "        visual_has_cls_token=False,\n",
    "        channels=1,\n",
    "        use_all_token_embeds=False,\n",
    "        downsample_image_embeds=False,\n",
    "        extra_latent_projection=False,\n",
    "        use_mlm=False\n",
    "    )\n",
    "    \n",
    "    if model_path and os.path.exists(model_path):\n",
    "        print(f\"Loading pretrained weights from {model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            # Check if checkpoint is nested\n",
    "            state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "            model.load_state_dict(state_dict, strict=False)  # Use strict=False to handle partial matches\n",
    "            print(\"✓ Pretrained weights loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load pretrained weights: {e}\")\n",
    "            print(\"Using randomly initialized model\")\n",
    "    else:\n",
    "        print(\"No pretrained weights specified, using randomly initialized model\")\n",
    "        print(\"Note: For meaningful features, you should use pretrained weights\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, biomed_model\n",
    "\n",
    "\n",
    "def get_text_embeddings(text_prompts, tokenizer, biomed_model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Get text embeddings using the BiomedVLP model.\n",
    "    \n",
    "    Args:\n",
    "        text_prompts (list): List of text strings\n",
    "        tokenizer: BiomedVLP tokenizer\n",
    "        biomed_model: BiomedVLP model\n",
    "        device (str): Device to use\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Text embeddings\n",
    "    \"\"\"\n",
    "    # Tokenize and compute the sentence embeddings\n",
    "    tokenizer_output = tokenizer.batch_encode_plus(\n",
    "        batch_text_or_text_pairs=text_prompts,\n",
    "        add_special_tokens=True,\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move tokenizer output to device\n",
    "    tokenizer_output = {k: v.to(device) for k, v in tokenizer_output.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = biomed_model.get_projected_text_embeddings(\n",
    "            input_ids=tokenizer_output['input_ids'],\n",
    "            attention_mask=tokenizer_output['attention_mask']\n",
    "        )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def compute_text_similarity(text_prompts, tokenizer, biomed_model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between text prompts.\n",
    "    \n",
    "    Args:\n",
    "        text_prompts (list): List of text strings\n",
    "        tokenizer: BiomedVLP tokenizer\n",
    "        biomed_model: BiomedVLP model\n",
    "        device (str): Device to use\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Similarity matrix\n",
    "    \"\"\"\n",
    "    embeddings = get_text_embeddings(text_prompts, tokenizer, biomed_model, device)\n",
    "    \n",
    "    # Compute the cosine similarity of sentence embeddings\n",
    "    sim = torch.mm(embeddings, embeddings.t())\n",
    "    return sim\n",
    "\n",
    "\n",
    "# Configuration\n",
    "model_path = r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\CT-CLIP\\CT_VocabFine_v2.pt\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "try:\n",
    "    model, tokenizer, biomed_model = load_ctclip_model(model_path, device)\n",
    "    print(\"✓ Model and tokenizer loaded successfully!\")\n",
    "    print(f\"✓ CT-CLIP model is on device: {next(model.parameters()).device}\")\n",
    "    print(f\"✓ BiomedVLP model is on device: {next(biomed_model.parameters()).device}\")\n",
    "    \n",
    "    # Example usage of text embeddings\n",
    "    text_prompts = [\n",
    "        \"There is no pneumothorax or pleural effusion\",\n",
    "        \"No pleural effusion or pneumothorax is seen\",\n",
    "        \"The extent of the pleural effusion is constant.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting text embeddings...\")\n",
    "    embeddings = get_text_embeddings(text_prompts, tokenizer, biomed_model, device)\n",
    "    print(f\"✓ Text embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = compute_text_similarity(text_prompts, tokenizer, biomed_model, device)\n",
    "    print(f\"✓ Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "    print(\"✓ Similarity matrix:\")\n",
    "    print(similarity_matrix)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "867671b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Generated 34 text prompts for 17 conditions\n",
      "✗ Error during feature extraction: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\20203686\\AppData\\Local\\Temp\\ipykernel_14352\\3604583878.py\", line 108, in <module>\n",
      "    results = extract_features(preprocessed_volume, model, tokenizer, biomed_model, device)\n",
      "  File \"C:\\Users\\20203686\\AppData\\Local\\Temp\\ipykernel_14352\\3604583878.py\", line 51, in extract_features\n",
      "    image_embedding = model.visual_transformer(ct_volume_tensor)  # Shape: (1, num_patches + 1, dim_image) or (1, dim_image)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\20203686\\Documents\\GitHub\\CT-CLIP\\CT_CLIP\\ct_clip\\ct_clip.py\", line 374, in forward\n",
      "    x = self.to_tokens(x)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\20203686\\AppData\\Local\\anaconda3\\envs\\MasterProject\\lib\\site-packages\\einops\\layers\\torch.py\", line 14, in forward\n",
      "    recipe = self._multirecipe[input.ndim]\n",
      "KeyError: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def extract_features(ct_volume_tensor, model, tokenizer, biomed_model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract features for specified medical conditions from a CT volume using CT-CLIP.\n",
    "    \n",
    "    Args:\n",
    "        ct_volume_tensor (torch.Tensor): Preprocessed CT volume tensor (1, 1, D, H, W)\n",
    "        model (CTCLIP): Loaded CT-CLIP model\n",
    "        tokenizer: BiomedVLP tokenizer\n",
    "        biomed_model: BiomedVLP model\n",
    "        device (str): Device to use ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with condition names as keys and binary labels (0 or 1) as values\n",
    "    \"\"\"\n",
    "    # Define the list of conditions based on training data\n",
    "    conditions = [\n",
    "        \"Arterial wall calcification\",\n",
    "        \"Cardiomegaly\",\n",
    "        \"Pericardial effusion\",\n",
    "        \"Coronary artery wall calcification\",\n",
    "        \"Hiatal hernia\",\n",
    "        \"Lymphadenopathy\",\n",
    "        \"Emphysema\",\n",
    "        \"Atelectasis\",\n",
    "        \"Lung nodule\",\n",
    "        \"Lung opacity\",\n",
    "        \"Pulmonary fibrotic sequela\",\n",
    "        \"Pleural effusion\",\n",
    "        \"Mosaic attenuation pattern\",\n",
    "        \"Peribronchial thickening\",\n",
    "        \"Consolidation\",\n",
    "        \"Bronchiectasis\",\n",
    "        \"Interlobular septal thickening\"\n",
    "    ]\n",
    "    \n",
    "    # Create text prompts for presence and absence of each condition\n",
    "    text_prompts = [f\"Presence of {condition}\" for condition in conditions] + \\\n",
    "                   [f\"No {condition}\" for condition in conditions]\n",
    "    \n",
    "    print(f\"Generated {len(text_prompts)} text prompts for {len(conditions)} conditions\")\n",
    "    \n",
    "    # Move CT volume tensor to device\n",
    "    ct_volume_tensor = ct_volume_tensor.to(device)\n",
    "    \n",
    "    # Generate CT volume embedding\n",
    "    with torch.no_grad():\n",
    "        # Use visual_transformer instead of image_encoder\n",
    "        image_embedding = model.visual_transformer(ct_volume_tensor)  # Shape: (1, num_patches + 1, dim_image) or (1, dim_image)\n",
    "        # If use_all_token_embeds is True, select non-CLS tokens or average\n",
    "        if model.use_all_token_embeds:\n",
    "            image_embedding = image_embedding[:, 1:] if model.visual_has_cls_token else image_embedding  # Exclude CLS token\n",
    "            image_embedding = torch.mean(image_embedding, dim=1)  # Average over patches\n",
    "        else:\n",
    "            image_embedding = image_embedding[:, 0] if model.visual_has_cls_token else image_embedding  # Use CLS token or full embedding\n",
    "        # Project to latent space\n",
    "        image_latent = model.to_visual_latent(image_embedding)  # Shape: (1, dim_latent)\n",
    "        image_latent = F.normalize(image_latent, dim=-1)  # L2 normalize\n",
    "    \n",
    "    print(f\"CT volume latent shape: {image_latent.shape}\")\n",
    "    \n",
    "    # Generate text embeddings\n",
    "    tokenizer_output = tokenizer.batch_encode_plus(\n",
    "        batch_text_or_text_pairs=text_prompts,\n",
    "        add_special_tokens=True,\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    tokenizer_output = {k: v.to(device) for k, v in tokenizer_output.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use BiomedVLP model for text embeddings\n",
    "        text_embeddings = biomed_model.get_projected_text_embeddings(\n",
    "            input_ids=tokenizer_output['input_ids'],\n",
    "            attention_mask=tokenizer_output['attention_mask']\n",
    "        )\n",
    "        # Project to latent space using CTCLIP's text projection\n",
    "        text_latents = model.to_text_latent(text_embeddings)  # Shape: (num_prompts, dim_latent)\n",
    "        text_latents = F.normalize(text_latents, dim=-1)  # L2 normalize\n",
    "    \n",
    "    print(f\"Text latents shape: {text_latents.shape}\")\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = F.cosine_similarity(image_latent, text_latents, dim=-1)  # Shape: (num_prompts,)\n",
    "    print(f\"Similarity scores shape: {similarities.shape}\")\n",
    "    \n",
    "    # Classify each condition based on similarity scores\n",
    "    results = {}\n",
    "    for i, condition in enumerate(conditions):\n",
    "        presence_score = similarities[i]\n",
    "        absence_score = similarities[i + len(conditions)]\n",
    "        results[condition] = 1 if presence_score > absence_score else 0\n",
    "        print(f\"{condition}: Presence score = {presence_score:.4f}, Absence score = {absence_score:.4f}, Predicted = {results[condition]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ensure required variables are available\n",
    "if 'preprocessed_volume' not in locals() or 'model' not in locals() or 'tokenizer' not in locals() or 'biomed_model' not in locals():\n",
    "    print(\"✗ Error: Required variables (preprocessed_volume, model, tokenizer, biomed_model) not found.\")\n",
    "    print(\"Please run the previous cells to load and preprocess the CT volume and model.\")\n",
    "else:\n",
    "    try:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {device}\")\n",
    "        results = extract_features(preprocessed_volume, model, tokenizer, biomed_model, device)\n",
    "        print(\"\\nFeature extraction results:\")\n",
    "        for condition, label in results.items():\n",
    "            print(f\"{condition}: {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during feature extraction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f7535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
