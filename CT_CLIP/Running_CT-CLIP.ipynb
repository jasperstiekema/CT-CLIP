{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d168aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clip = CTCLIP(\n",
    "    image_encoder = image_encoder,\n",
    "    text_encoder = text_encoder,\n",
    "    dim_image = 2097152,\n",
    "    dim_text = 768,\n",
    "    dim_latent = 512,\n",
    "    extra_latent_projection = False,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n",
    "    use_mlm = False,\n",
    "    downsample_image_embeds = False,\n",
    "    use_all_token_embeds = False\n",
    "\n",
    ")\n",
    "\n",
    "#Load the pretrained weights during inference\n",
    "\n",
    "clip.load(\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\CT-CLIP\\CT_VocabFine_v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf42aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\20203686\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ca079",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\TEST CT SCANS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e783e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.ndimage import zoom\n",
    "from torchvision import transforms as T\n",
    "\n",
    "def preprocess_dicom_directory(directory, image_size=256):\n",
    "    \"\"\"\n",
    "    Load DICOM files from a directory, stack into a 3D volume, and preprocess for CT-CLIP.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to directory containing .dcm files.\n",
    "        image_size (int): Target size for resizing (default: 256).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed CT volume with shape (1, 3, image_size, image_size, image_size).\n",
    "    \"\"\"\n",
    "    # Read all .dcm files from the directory\n",
    "    dicom_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.dcm')]\n",
    "    dicom_files.sort()  # Ensure slices are in order (assuming filename sorting is sufficient)\n",
    "\n",
    "    # Load DICOM slices\n",
    "    slices = [pydicom.dcmread(f).pixel_array for f in dicom_files]\n",
    "    volume = np.stack(slices, axis=-1)  # Shape: (height, width, depth)\n",
    "\n",
    "    # Normalize pixel values (HU to [0, 1] or model-specific range)\n",
    "    volume = (volume - np.min(volume)) / (np.max(volume) - np.min(volume) + 1e-6)\n",
    "\n",
    "    # Resize to target size (image_size, image_size, image_size)\n",
    "    zoom_factors = (image_size / volume.shape[0], image_size / volume.shape[1], image_size / volume.shape[2])\n",
    "    volume = zoom(volume, zoom_factors, order=1)  # Linear interpolation\n",
    "\n",
    "    # Convert to tensor and add channel dimension\n",
    "    volume = torch.tensor(volume, dtype=torch.float32)\n",
    "    if volume.ndim == 3:\n",
    "        volume = volume.unsqueeze(0)  # Add channel dim: (1, H, W, D)\n",
    "        volume = volume.repeat(3, 1, 1, 1)  # Repeat to simulate 3 channels: (3, H, W, D)\n",
    "    else:\n",
    "        volume = volume.permute(3, 0, 1, 2)  # (H, W, D, C) -> (C, H, W, D)\n",
    "\n",
    "    # Apply normalization (as defined in visual_ssl.py)\n",
    "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    volume = normalize(volume)\n",
    "\n",
    "    # Add batch dimension\n",
    "    volume = volume.unsqueeze(0)  # Shape: (1, 3, image_size, image_size, image_size)\n",
    "    return volume\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f20bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type cxr-bert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at microsoft/BiomedVLP-CXR-BERT-specialized and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3623878656 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     17\u001b[0m text_transformer \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/BiomedVLP-CXR-BERT-specialized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize vision transformer\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# The shape mismatch for to_visual_latent.weight ([512, 294912]) suggests a larger output dimension\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate expected dim_image: 294912 / (256 // 32) ** 2 = 294912 / 64 = 4608 tokens\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# This implies a different patch size or additional features\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m vision_transformer \u001b[38;5;241m=\u001b[39m VisionTransformer(\n\u001b[0;32m     24\u001b[0m     dim\u001b[38;5;241m=\u001b[39mdim_image,\n\u001b[0;32m     25\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mvisual_image_size,\n\u001b[0;32m     26\u001b[0m     patch_size\u001b[38;5;241m=\u001b[39mvisual_patch_size,  \u001b[38;5;66;03m# May need adjustment (e.g., smaller patch size)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     channels\u001b[38;5;241m=\u001b[39mchannels,\n\u001b[0;32m     28\u001b[0m     depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m     29\u001b[0m     heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     30\u001b[0m     dim_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     31\u001b[0m     patch_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize CT-CLIP\u001b[39;00m\n\u001b[0;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m CTCLIP(\n\u001b[0;32m     36\u001b[0m     image_encoder\u001b[38;5;241m=\u001b[39mvision_transformer,\n\u001b[0;32m     37\u001b[0m     text_encoder\u001b[38;5;241m=\u001b[39mtext_transformer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     multiview_loss_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     52\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20203686\\Documents\\GitHub\\CT-CLIP\\CT_CLIP\\ct_clip\\ct_clip.py:353\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[1;34m(self, dim, image_size, patch_size, channels, patch_dropout, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m (image_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m patch_size) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    349\u001b[0m patch_dim \u001b[38;5;241m=\u001b[39m channels \u001b[38;5;241m*\u001b[39m patch_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m    352\u001b[0m     Rearrange(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c (h p1) (w p2) -> b (h w) (p1 p2 c)\u001b[39m\u001b[38;5;124m'\u001b[39m, p1 \u001b[38;5;241m=\u001b[39m patch_size, p2 \u001b[38;5;241m=\u001b[39m patch_size),\n\u001b[1;32m--> 353\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(patch_dim, dim)\n\u001b[0;32m    354\u001b[0m )\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(num_patches, dim)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_dropout \u001b[38;5;241m=\u001b[39m PatchDropout(patch_dropout)\n",
      "File \u001b[1;32mc:\\Users\\20203686\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3623878656 bytes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ct_clip.ct_clip import CTCLIP, VisionTransformer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Define model parameters\n",
    "dim_text = 768  # Matches BiomedVLP-CXR-BERT-specialized output dimension\n",
    "dim_image = 294912  # Adjust based on VisionTransformer output\n",
    "dim_latent = 512\n",
    "num_text_tokens = 28897\n",
    "text_seq_len = 512\n",
    "visual_image_size = 256\n",
    "visual_patch_size = 32  # Adjust if needed (see below)\n",
    "channels = 3\n",
    "\n",
    "# Initialize text transformer using BertModel\n",
    "text_transformer = BertModel.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized')\n",
    "\n",
    "# Initialize vision transformer\n",
    "# The shape mismatch for to_visual_latent.weight ([512, 294912]) suggests a larger output dimension\n",
    "# Calculate expected dim_image: 294912 / (256 // 32) ** 2 = 294912 / 64 = 4608 tokens\n",
    "# This implies a different patch size or additional features\n",
    "vision_transformer = VisionTransformer(\n",
    "    dim=dim_image,\n",
    "    image_size=visual_image_size,\n",
    "    patch_size=visual_patch_size,  # May need adjustment (e.g., smaller patch size)\n",
    "    channels=channels,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    dim_head=64,\n",
    "    patch_dropout=0.5\n",
    ")\n",
    "\n",
    "# Initialize CT-CLIP\n",
    "model = CTCLIP(\n",
    "    image_encoder=vision_transformer,\n",
    "    text_encoder=text_transformer,\n",
    "    dim_text=dim_text,\n",
    "    dim_image=dim_image,\n",
    "    dim_latent=dim_latent,\n",
    "    num_text_tokens=num_text_tokens,\n",
    "    text_seq_len=text_seq_len,\n",
    "    visual_image_size=visual_image_size,\n",
    "    visual_patch_size=visual_patch_size,\n",
    "    channels=channels,\n",
    "    use_mlm=False,\n",
    "    use_visual_ssl=False,\n",
    "    visual_ssl_type='simsiam',\n",
    "    text_ssl_loss_weight=0.05,\n",
    "    image_ssl_loss_weight=0.05,\n",
    "    multiview_loss_weight=0.1\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load pretrained weights\n",
    "state_dict = torch.load(\n",
    "    r\"C:\\Users\\20203686\\OneDrive - TU Eindhoven\\TUe\\Master\\Year 2\\MASTER PROJECT\\CT-CLIP\\CT_VocabFine_v2.pt\",\n",
    "    map_location=device\n",
    ")\n",
    "model.load_state_dict(state_dict, strict=False)  # Use strict=False to ignore missing/unexpected keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b50850",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b14fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
